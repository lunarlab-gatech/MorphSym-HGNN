<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=i5PF63IAAAAJ&hl=en" target="_blank">Daniel Butterfield</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=Sdbn4VYAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Sandilya Sai Garimella</a>,</span>
                  <span class="author-block">
                    <a href="https://sciprofiles.com/profile/3891456" target="_blank">Nai-Jen Cheng</a>,</span>
                  </span>
                  <span class="author-block"></span>
                    <a href="https://ganlumomo.github.io/" target="_blank">Lu Gan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Georgia Institute of Technology<br>Submitted to ICRA 2025</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2409.11146" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!--<span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/lunarlab-gatech/Morphology-Informed-HGNN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2409.11146" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body is-centered">
      <!-- Your image here -->
      <img src="static/images/figure1.png" alt="Figure 1; Visualization of our MI-HGNN for the Mini-Cheetah robot as an example." width="500"/>
      <h2 class="subtitle has-text-centered">
        Visualization of our MI-HGNN for the Mini-Cheetah robot as an example. The structure and connectivity of our graph is constructed from the robot morphology. Local sensor measurements are embedded into the corresponding node to predict contact information at the foot node.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN) for learning-based contact perception. The architecture and connectivity of the MI-HGNN are constructed from the robot morphology, in which nodes and edges are robot joints and links, respectively. By incorporating the morphology-informed constraints into a neural network, we improve a learning-based approach using model-based knowledge. We apply the proposed MI-HGNN to two contact perception problems, and conduct extensive experiments using both real-world and simulated data collected using two quadruped robots. Our experiments demonstrate the superiority of our method in terms of effectiveness, generalization ability, model efficiency, and sample efficiency. Our MI-HGNN improved the performance of a state-of-the-art model that leverages robot morphological symmetry by 8.4% with only 0.21% of its parameters. Although MI-HGNN is applied to contact perception problems for legged robots in this work, it can be seamlessly applied to other types of multi-body dynamical systems and has the potential to improve other robot learning frameworks. Our code is made publicly available at <a href="https://github.com/lunarlab-gatech/Morphology-Informed-HGNN" target="_blank">https://github.com/lunarlab-gatech/Morphology-Informed-HGNN</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/figure2.png" alt="Figure 2; Overview of the proposed MI-HGNN for legged robot contact perception problems."/>
        <h2 class="subtitle has-text-centered top-margin">
          Overview of the proposed MI-HGNN for legged robot contact perception problems. Our MI-HGNN is constructed from a robot kinematic structure where nodes are joints and edges are links. Proprioceptive sensor measurements acquired at each local frame are embedded into the corresponding node through a heterogeneous encoder, and fused via several layers of Message Passing. A foot decoder attached to the foot node exacts the contact information during inference.
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <h1 class="title has-text-centered">
          How does MI-HGNN work?
        </h1>
        <!-- Your image here -->
         <h2 class="subtitle has-text-centered">
          <strong>A data correlation aspect:</strong> MI-HGNN constrains the learning problem based on the robot's morphology and explicitly captures the correlation and causality between inputs. This configuration mirrors the flow of information in a robotic system, as represented by a kinematic tree. Message passing between nodes is influenced by intermediate nodes, resembling the physical laws of the robot's system, thereby embedding causality into the message-passing process. As a result, our neural network achieves comparable performance to other methods with fewer parameters, as it does not need to learn the implicit relationships between data.
          <br><br>
          <strong>A symmetry aspect:</strong> As the Message-Passing layer M varies only according to the edge types R, the learned weights W are identical between all morphologically-identical limbs of the robotic platform. In addition to reducing the complexity of the search space and lowering model sizes, these constraints help us learn a model that is shared by the four legs for quadruped robots. Although this overconstrains the learning problem, empirically we find that this weight sharing allows the model to quickly learn a more generalizable function when compared to completely unconstrained learning models, leading to better performance.
         </h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <h1 class="title has-text-centered">
          What are the benefits of MI-HGNN?
        </h1>
        <!-- Your image here -->
         <h2 class="subtitle has-text-centered">
          <strong>Performance:</strong> 
          <br><br>
          <strong>Generalization:</strong> For the real-world Contact Detection task, the test set includes recordings on unseen ground types and gait types; and for the simulated GRF Estimation task, the test set includes unseen friction, speed, and terrain parameters. Therefore, our improved performance also demonstrates the generalization ability of our model on out-of-distribution data. 
          <br><br>
          <strong>Model Efficiency:</strong> Our MI-HGNN is not sensitive to the hyperparameters. Using our smallest MI-HGNN model, we reduce the model parameters from ECNN by a factor of <strong>482</strong> and still improve its performance by 8.4%, when compared with the ECNN accuracy from Fig. 3 (a).
          <br><br>
          <strong>Sample Efficiency:</strong>  Overall, we find that the performance of our model does not drop significantly until the number of training samples is reduced to <strong>10%</strong> of the entire training set. In addition, competitive results can still
          be achieved by our model using only <strong>2.5%</strong>of the training set, i.e., 15863 training samples.
         </h2>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small"></section>
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <img src="static/images/figure3.png" alt="Figure 3; Contact detection results on the real-world Mini-Cheetah contact dataset."/>
        <h2 class="subtitle has-text-centered">
          Contact detection results on the real-world Mini-Cheetah contact dataset: (a) classification performance of four models on the unseen test set, trained with the entire training set. The mean and standard deviation across 8 runs are reported. (b) sample efficiency evaluation for all models.
          <br> <br>
          In terms of contact state accuracy, our MI-HGNN achieves a mean accuracy of 87.7% across 8 runs, improving the performance of the
          second best model (i.e., ECNN with 78.8% mean accuracy) by 11.3%. Compared with the morphology-agnostic CNN model (with 71.8% mean accuracy), our model outperforms it by 22.1%. 
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/figure5.png" alt="Evaluation of all model types on a sub sequence of the “Unseen All” test sequence for the GRF estimation task."/>
        <h2 class="subtitle has-text-centered">
          Evaluation of all model types on a sub sequence of the “Unseen All” test sequence for the GRF estimation task, which includes an unseen friction coefficient ( ˙x = 0.5), unseen speed (v = 1.0), and unseen terrain (rough).
        </h2>
      </div>
    </div>
  </div>
</section>



<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            Your video file here 
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            Your video file here 
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
             Your video file here 
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{butterfield2024mi,
  title={MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception},
  author={Butterfield, Daniel and Garimella, Sandilya Sai and Cheng, Nai-Jen and Gan, Lu},
  journal={arXiv preprint arXiv:2409.11146},
  year={2024},
  eprint={2409.11146},
  url={https://arxiv.org/abs/2409.11146},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
            This page was built upon the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, but must link back to these pages and the license.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
